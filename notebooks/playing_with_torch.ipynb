{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('disaster': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8f88205dfb88151b33a02f70cebedd6a75a91865ead0e3b8b83d042dc7ae8060"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Playing around with Torch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tensor([[1, 1, 1],\n        [1, 1, 1],\n        [1, 1, 1]])"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tensor([[0.6926, 0.3047, 0.3078],\n        [0.1662, 0.5870, 0.3211],\n        [0.8636, 0.0730, 0.2914]])"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display(torch.ones_like(x_np))\n",
    "display(torch.ones_like(x_np, dtype=torch.float))\n",
    "\n",
    "display(torch.rand_like(x_np, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tensor([[0.8827, 0.6595, 0.4358],\n        [0.4490, 0.5012, 0.3638],\n        [0.3348, 0.9670, 0.0726]])"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])"
     },
     "metadata": {}
    }
   ],
   "source": [
    "shape = (3,3,)\n",
    "display(torch.rand(shape))\n",
    "display(torch.ones(shape))\n",
    "display(torch.zeros(shape))\n",
    "\n",
    "rand_tensor = torch.rand(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "torch.Size([3, 3])"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "torch.float32"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display(rand_tensor.shape)\n",
    "display(rand_tensor.device)\n",
    "display(rand_tensor.dtype)"
   ]
  },
  {
   "source": [
    "Go to [here](https://pytorch.org/docs/stable/torch.html) for a full list of torch operations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "rand_tensor.numel() # total number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(tensor([[1, 2, 3]]), tensor([[4, 5, 6]]), tensor([[7, 8, 9]]))"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]])"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tensor([[[1, 4, 7],\n         [2, 5, 8],\n         [3, 6, 9]]])"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tensor([[1, 2, 3]])"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]])"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tensor([5, 6, 7, 8, 9])"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]]), tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]))"
     },
     "metadata": {}
    }
   ],
   "source": [
    "tmp = torch.arange(1, 10, 1).reshape(3,3)\n",
    "display(tmp)\n",
    "chunked = tmp.chunk(3)\n",
    "display(chunked)\n",
    "\n",
    "display(torch.column_stack(chunked))\n",
    "display(torch.cat(chunked))\n",
    "display(torch.dstack(chunked))\n",
    "display(torch.dstack(chunked)[:,:,0])\n",
    "display(torch.vstack(chunked))\n",
    "display(torch.hstack(chunked))\n",
    "display(tmp.masked_select(tmp >4))\n",
    "display((torch.hstack(chunked), torch.hstack(chunked).squeeze()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "source": [
    "What the grad arguments mean, `require_grad` specifies whther to keep track of the operations the variables are involved in."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([60., 97.], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "Q = 3*a**3 + b**2\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1., 1.])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "external_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "all(a.grad == 9*a**2)"
   ]
  },
  {
   "source": [
    "Fromt pytorch tutorial\n",
    "\n",
    "**A Recipe for a neural net training**\n",
    "- model with some learnable params(or weights)\n",
    "- iterate over a dataset of inputs\n",
    "- process input through the network\n",
    "- compute the loss\n",
    "- propagate grads back into network parameters\n",
    "- update the weights using a simple rule(like weight=weighte - lr* gradient)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- model is a class with `__init__` , and `forward`, and if any other thing\n",
    "- you can `print` the model or see the `.parameters()`\n",
    "- zero the grad \n",
    "- and do backward\n",
    "\n",
    "Recap:\n",
    "- `torch.Tensor` - A multi-dimensional array with support for *autograd* operations like `backward()`. Also *holds* the *gradient* w.r.t. the tensor.\n",
    "- `nn.Module` - Neural network module. Convenient way of encapsulating *parameters*, with *helpers* for moving them to *GPU*, *exporting*, *loading*, etc.\n",
    "- `nn.Parameter` - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module.\n",
    "- `autograd.Function` - Implements `forward` and `backward` definitions of an autograd operation. Every Tensor operation creates at least a single Function node that connects to functions that created a Tensor and encodes its history.\n",
    "\n",
    "then define a loss\n",
    "- make sure output and the target are of the same shape\n",
    "- use a loss function on them\n",
    "\n",
    "Backprop:\n",
    "- zero the gradient buffers for all params `zero_grad()`\n",
    "- and do `backward()`\n",
    "\n",
    "Now update params\n",
    "- torch has an optim that helps with that with `step`\n",
    "    - so optimizer definition like `optimizer = optim.SGD(...)`\n",
    "    - `optimizer.zero_grad()`\n",
    "    - create output from the model given the input output = model(input)\n",
    "    - given the loss criterion define loss, `loss = criterion(output, target)`\n",
    "    - do backward `loss.backward()`\n",
    "    - update the params `optimizer.step()`\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}